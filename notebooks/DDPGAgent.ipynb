{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.agents import Agent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get the environment and extract the number of actions.\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.wrappers import Monitor\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "env = OpenAIGym(\n",
    "        gym_id=ENV_NAME,\n",
    "        monitor=None,\n",
    "        monitor_safe=None,\n",
    "        monitor_video=None,\n",
    "        visualize=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = env.actions['num_actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_states = env.states['shape'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the specifications of the DDPG agent according to the original paper\n",
    "agent = {\n",
    "    'actions_exploration': {'mu': 0.0,\n",
    "                            'sigma': 0.3,\n",
    "                            'theta': 0.15,\n",
    "                            'type': 'ornstein_uhlenbeck'},\n",
    "    'critic_network': {'size_t0': 64, \n",
    "                       'size_t1': 64},\n",
    "    'critic_optimizer': {'learning_rate': 0.001, \n",
    "                         'type': 'adam'},\n",
    "    'discount': 0.99,\n",
    "    'entropy_regularization': None,\n",
    "    'execution': {'distributed_spec': None,\n",
    "                  'session_config': None,\n",
    "                  'type': 'single'},\n",
    "    'memory': {'capacity': 100000, \n",
    "               'include_next_states': True, \n",
    "               'type': 'replay'},\n",
    "    'optimizer': {'learning_rate': 0.0001, \n",
    "               'type': 'adam'},\n",
    "    'saver': {'directory': None,\n",
    "              'seconds': 600},\n",
    "    'summarizer': {'directory': None,\n",
    "                   'labels': [],\n",
    "                   'seconds': 120},\n",
    "    'target_sync_frequency': 1,\n",
    "    'target_update_weight': 0.999,\n",
    "    'type': 'ddpg_agent',\n",
    "    'update_mode': {'batch_size': 64, \n",
    "                    'frequency': 64, \n",
    "                    'unit': 'timesteps'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the networks to be used in DDPG. Although DDPG uses two networks (actor-critic), we'll use the\n",
    "# same configuration for both as per the original paper\n",
    "network = [  \n",
    "             {'size': 64, 'type': 'linear'},\n",
    "             {'layer': 'batch_normalization', 'type': 'tf_layer'},\n",
    "             {'name': 'relu', 'type': 'nonlinearity'},\n",
    "             {'size': 64, 'type': 'linear'},\n",
    "             {'layer': 'batch_normalization', 'type': 'tf_layer'},\n",
    "             {'name': 'relu', 'type': 'nonlinearity'},\n",
    "             {'activation': None, 'size': 64, 'type': 'dense'}\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorforce.contrib.openai_gym.OpenAIGym at 0x7feb4004c2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/anmol/tensorforce/tensorforce/core/distributions/categorical.py:66: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "agent = Agent.from_spec(\n",
    "        spec=agent,\n",
    "        kwargs=dict(\n",
    "            states=env.states,\n",
    "            actions=env.actions,\n",
    "            network=network,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorforce.agents.ddpg_agent.DDPGAgent at 0x7feb3ffe7588>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DDPGAgent for Environment 'OpenAIGym(CartPole-v0)'\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "runner = Runner(\n",
    "        agent=agent,\n",
    "        environment=env,\n",
    "        repeat_actions=1\n",
    "    )\n",
    "\n",
    "if DEBUG:  # TODO: Timestep-based reporting\n",
    "    report_episodes = 1\n",
    "else:\n",
    "    report_episodes = 100\n",
    "\n",
    "print(\"Starting {agent} for Environment '{env}'\".format(agent=agent, env=env))\n",
    "\n",
    "def episode_finished(r, id_):\n",
    "    if r.episode % report_episodes == 0:\n",
    "        steps_per_second = r.timestep / (time.time() - r.start_time)\n",
    "        print(\"Finished episode {:d} after {:d} timesteps. Steps Per Second {:0.2f}\".format(\n",
    "            r.agent.episode, r.episode_timestep, steps_per_second\n",
    "        ))\n",
    "        print(\"Episode reward: {}\".format(r.episode_rewards[-1]))\n",
    "        print(\"Average of last 500 rewards: {:0.2f}\".\n",
    "                    format(sum(r.episode_rewards[-500:]) / min(500, len(r.episode_rewards))))\n",
    "        print(\"Average of last 100 rewards: {:0.2f}\".\n",
    "                    format(sum(r.episode_rewards[-100:]) / min(100, len(r.episode_rewards))))\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1 after 10 timesteps. Steps Per Second 7.88\n",
      "Episode reward: 10.0\n",
      "Average of last 500 rewards: 10.00\n",
      "Average of last 100 rewards: 10.00\n",
      "Finished episode 2 after 14 timesteps. Steps Per Second 18.31\n",
      "Episode reward: 14.0\n",
      "Average of last 500 rewards: 12.00\n",
      "Average of last 100 rewards: 12.00\n",
      "Finished episode 3 after 12 timesteps. Steps Per Second 23.84\n",
      "Episode reward: 12.0\n",
      "Average of last 500 rewards: 12.00\n",
      "Average of last 100 rewards: 12.00\n",
      "Finished episode 4 after 9 timesteps. Steps Per Second 27.06\n",
      "Episode reward: 9.0\n",
      "Average of last 500 rewards: 11.25\n",
      "Average of last 100 rewards: 11.25\n",
      "Finished episode 5 after 11 timesteps. Steps Per Second 30.37\n",
      "Episode reward: 11.0\n",
      "Average of last 500 rewards: 11.20\n",
      "Average of last 100 rewards: 11.20\n",
      "Finished episode 6 after 9 timesteps. Steps Per Second 32.57\n",
      "Episode reward: 9.0\n",
      "Average of last 500 rewards: 10.83\n",
      "Average of last 100 rewards: 10.83\n",
      "Finished episode 7 after 9 timesteps. Steps Per Second 34.53\n",
      "Episode reward: 9.0\n",
      "Average of last 500 rewards: 10.57\n",
      "Average of last 100 rewards: 10.57\n",
      "Finished episode 8 after 9 timesteps. Steps Per Second 36.17\n",
      "Episode reward: 9.0\n",
      "Average of last 500 rewards: 10.38\n",
      "Average of last 100 rewards: 10.38\n",
      "Finished episode 9 after 9 timesteps. Steps Per Second 37.66\n",
      "Episode reward: 9.0\n",
      "Average of last 500 rewards: 10.22\n",
      "Average of last 100 rewards: 10.22\n",
      "Finished episode 10 after 9 timesteps. Steps Per Second 38.93\n",
      "Episode reward: 9.0\n",
      "Average of last 500 rewards: 10.10\n",
      "Average of last 100 rewards: 10.10\n"
     ]
    }
   ],
   "source": [
    "runner.run(\n",
    "        num_timesteps=None,\n",
    "        num_episodes=10,\n",
    "        max_episode_timesteps=None,\n",
    "        deterministic=False,\n",
    "        episode_finished=episode_finished\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning finished. Total episodes: 10\n"
     ]
    }
   ],
   "source": [
    "runner.close()\n",
    "\n",
    "print(\"Learning finished. Total episodes: {ep}\".format(ep=runner.agent.episode))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
